# -*- coding: utf-8 -*-
"""Strategy01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zuNhtNf6WC5CxVlVBSVkMxGcvn_hOCdF
"""

!pip install ta

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

import yfinance as yf
from datetime import datetime, timedelta
import functions_algtg

full_data = functions_algtg.read_data()

full_data.columns = ['Close', 'High', 'Low', 'Open', 'Volume']

functions_algtg.technical_indicators

today = datetime.strptime("2025-07-29", "%Y-%m-%d")
train_end_date = today - timedelta(days=365)

train_data = full_data[full_data.index < train_end_date]
test_data = full_data[full_data.index >= train_end_date]

train_data_ti = functions_algtg.technical_indicators(train_data, short_window = 5, long_window = 26)

train_data_ti.head()

train_data_ti['Return'] = (train_data_ti['Close'] - train_data_ti['Open']) / train_data_ti['Open']

train_data_ti['Intraday_trade'] = 0
train_data_ti.loc[train_data_ti['Return'] > 0.02, 'Intraday_trade'] = 1

train_data_ti['Intraday_trade'].value_counts()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import precision_score, make_scorer, confusion_matrix, classification_report, roc_auc_score
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, Tuple, List, Optional

# --- Helper Functions ---

def _preprocess_features(X: pd.DataFrame) -> pd.DataFrame:
    """
    Preprocesses the features DataFrame to ensure all columns are numeric.
    Attempts to convert object columns to numeric, dropping them if conversion fails.
    Fills NaN values with 0 after conversion.

    Args:
        X (pd.DataFrame): Input features DataFrame.

    Returns:
        pd.DataFrame: Preprocessed DataFrame with only numeric columns.
    """
    print("Starting feature preprocessing to ensure numeric dtypes...")
    X_processed = X.copy()
    columns_to_drop = []

    for col in X_processed.columns:
        if X_processed[col].dtype == 'object':
            # Attempt to convert to numeric, coercing errors to NaN
            X_processed[col] = pd.to_numeric(X_processed[col], errors='coerce')
            # If after coercion, the column is all NaN, it means it couldn't be converted
            if X_processed[col].isnull().all():
                print(f"Warning: Column '{col}' contains non-numeric data and cannot be converted. Dropping column.")
                columns_to_drop.append(col)
            else:
                # Fill NaNs that resulted from coercion (e.g., if some values were non-numeric)
                # A common strategy is to fill with 0 or the mean/median, depending on context.
                # For simplicity, filling with 0 here.
                X_processed[col] = X_processed[col].fillna(0)
        elif X_processed[col].isnull().any():
            # If a numeric column has NaNs, fill them.
            # Again, filling with 0 for simplicity. Consider mean/median imputation.
            print(f"Warning: Column '{col}' contains NaN values. Filling with 0.")
            X_processed[col] = X_processed[col].fillna(0)

    if columns_to_drop:
        X_processed = X_processed.drop(columns=columns_to_drop)

    # Final check for any remaining non-numeric columns (shouldn't happen if logic is correct)
    non_numeric_cols = X_processed.select_dtypes(include=['object']).columns
    if not non_numeric_cols.empty:
        print(f"Error: Unexpected non-numeric columns remaining after preprocessing: {list(non_numeric_cols)}. Dropping them.")
        X_processed = X_processed.drop(columns=non_numeric_cols)

    print("Feature preprocessing complete.")
    return X_processed


def split_data(X: pd.DataFrame, y: pd.Series, test_size: float = 0.2, val_size: float = 0.2, random_state: Optional[int] = None) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:
    """
    Splits the input time-series data into training, validation, and test sets chronologically.
    Assumes the DataFrame index represents time in increasing order.

    Args:
        X (pd.DataFrame): Features DataFrame with a time-based index.
        y (pd.Series): Target Series with a time-based index.
        test_size (float): Proportion of the dataset to include in the test split (default: 0.2).
        val_size (float): Proportion of the dataset to include in the validation split (default: 0.2).
        random_state (Optional[int]): Not directly used for splitting order, but can be used for
                                      any internal shuffling if needed (though not typical for time series).

    Returns:
        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:
            X_train, X_val, X_test, y_train, y_val, y_test
    """
    # Ensure data is sorted by index (assumed to be time) to maintain chronological order
    X_sorted = X.sort_index()
    y_sorted = y.sort_index()

    total_samples = len(X_sorted)
    test_split_idx = int(total_samples * (1 - test_size))
    val_split_idx = int(total_samples * (1 - test_size - val_size))

    # Split into train, validation, and test sets chronologically
    X_train = X_sorted.iloc[:val_split_idx]
    y_train = y_sorted.iloc[:val_split_idx]

    X_val = X_sorted.iloc[val_split_idx:test_split_idx]
    y_val = y_sorted.iloc[val_split_idx:test_split_idx]

    X_test = X_sorted.iloc[test_split_idx:]
    y_test = y_sorted.iloc[test_split_idx:]

    print(f"Data split (Time Series): Train ({len(X_train)}), Val ({len(X_val)}), Test ({len(X_test)}) samples.")
    return X_train, X_val, X_test, y_train, y_val, y_test

def precision_coverage_scorer(y_true: np.ndarray, y_pred_proba: np.ndarray, threshold: float) -> float:
    """
    Custom scorer to calculate precision * coverage for a given threshold.

    Args:
        y_true (np.ndarray): True binary labels.
        y_pred_proba (np.ndarray): Predicted probabilities for the positive class.
        threshold (float): Probability threshold above which a prediction is considered positive.

    Returns:
        float: Precision * Coverage score. Returns 0 if no confident predictions are made.
    """
    # Identify samples where the model is confident (probability >= threshold)
    confident_indices = (y_pred_proba >= threshold) | (y_pred_proba <= (1 - threshold))

    if not np.any(confident_indices):
        return 0.0 # No confident predictions, so coverage is 0.

    # Filter true labels and predicted probabilities for confident samples only
    y_true_confident = y_true[confident_indices]
    y_pred_confident_binary = (y_pred_proba[confident_indices] >= 0.5).astype(int) # Convert to binary for precision

    if len(y_true_confident) == 0:
        return 0.0 # Should not happen if confident_indices is not empty, but good for safety

    # Calculate precision for the confident predictions
    # Use zero_division=0 to avoid warnings if no positive predictions are made
    precision = precision_score(y_true_confident, y_pred_confident_binary, zero_division=0)

    # Calculate coverage
    coverage = np.sum(confident_indices) / len(y_true)

    return precision * coverage

def train_and_tune_xgboost(
    X_train: pd.DataFrame, y_train: pd.Series,
    X_val: pd.DataFrame, y_val: pd.Series,
    param_grid: Dict[str, Any],
    random_state: Optional[int] = None
) -> xgb.Booster:
    """
    Trains and tunes an XGBoost model using GridSearchCV.

    Args:
        X_train (pd.DataFrame): Training features.
        y_train (pd.Series): Training target.
        X_val (pd.DataFrame): Validation features (used for early stopping or evaluation).
        y_val (pd.Series): Validation target.
        param_grid (Dict[str, Any]): Dictionary with parameters names (str) as keys
                                      and lists of parameter settings to try as values.
        random_state (Optional[int]): Random state for reproducibility.

    Returns:
        xgb.Booster: The best trained XGBoost model.
    """
    print("Starting XGBoost hyperparameter tuning...")

    # Initialize XGBoost Classifier
    # Use `objective='binary:logistic'` for binary classification and `eval_metric='logloss'`
    # for evaluation during training.
    # `use_label_encoder=False` is recommended to avoid deprecation warnings.
    xgb_model = xgb.XGBClassifier(
        objective='binary:logistic',
        eval_metric='logloss',
        use_label_encoder=False,
        random_state=random_state
    )

    # Define a custom scorer for GridSearchCV (e.g., precision, recall, f1)
    # For hyperparameter tuning, we typically optimize a standard metric on the validation set.
    # For this example, let's stick to 'roc_auc' as a common choice for binary classification.
    # The precision*coverage optimization comes *after* model tuning.
    scorer = make_scorer(precision_score, average='binary', zero_division=0)

    # Set up GridSearchCV
    grid_search = GridSearchCV(
        estimator=xgb_model,
        param_grid=param_grid,
        scoring=scorer,
        cv=3,  # Cross-validation folds
        verbose=1,
        n_jobs=-1  # Use all available cores
    )

    # Fit GridSearchCV to the training data
    # Note: For hyperparameter tuning, we fit on X_train, y_train.
    # X_val, y_val are used for the confidence threshold optimization.
    grid_search.fit(X_train, y_train)

    print(f"Best parameters found: {grid_search.best_params_}")
    print(f"Best cross-validation score: {grid_search.best_score_:.4f}")

    # Return the best estimator found by GridSearchCV
    return grid_search.best_estimator_

def optimize_confidence_threshold(
    model: xgb.Booster,
    X_val: pd.DataFrame,
    y_val: pd.Series,
    num_thresholds: int = 100
) -> float:
    """
    Optimizes the confidence threshold based on precision * coverage on the validation set.

    Args:
        model (xgb.Booster): The trained XGBoost model.
        X_val (pd.DataFrame): Validation features.
        y_val (pd.Series): Validation target.
        num_thresholds (int): Number of thresholds to evaluate between 0 and 1.

    Returns:
        float: The optimal confidence threshold.
    """
    print("Optimizing confidence threshold...")
    y_pred_proba = model.predict_proba(X_val)[:, 1]  # Probabilities for the positive class

    thresholds = np.linspace(0.01, 0.99, num_thresholds) # Avoid 0 and 1 directly
    best_score = -1
    best_threshold = 0.5 # Default if no better threshold is found

    for threshold in thresholds:
        score = precision_coverage_scorer(y_val.values, y_pred_proba, threshold)
        if score > best_score:
            best_score = score
            best_threshold = threshold

    print(f"Optimal confidence threshold: {best_threshold:.4f} with score: {best_score:.4f}")
    return best_threshold

def make_confident_predictions(
    model: xgb.Booster,
    X: pd.DataFrame,
    y: pd.Series,
    confidence_threshold: float
) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series, pd.Series]:
    """
    Makes high-confidence predictions and identifies unclassified data.

    Args:
        model (xgb.Booster): The trained XGBoost model.
        X (pd.DataFrame): Features to predict on.
        y (pd.Series): True labels for the features.
        confidence_threshold (float): The threshold for high-confidence predictions.

    Returns:
        Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series, pd.Series]:
            X_classified, y_classified, X_unclassified, y_unclassified, y_pred_confident
    """
    y_pred_proba = model.predict_proba(X)[:, 1] # Probabilities for the positive class

    # Identify confident predictions: where probability is either very high or very low
    # i.e., prob > threshold OR prob < (1 - threshold)
    confident_indices = (y_pred_proba >= confidence_threshold) | (y_pred_proba <= (1 - confidence_threshold))

    # Classified data
    X_classified = X[confident_indices]
    y_classified = y[confident_indices]
    # Ensure y_pred_confident is a Series for consistent concatenation later
    y_pred_confident = pd.Series((y_pred_proba[confident_indices] >= 0.5).astype(int), index=y_classified.index)

    # Unclassified data
    X_unclassified = X[~confident_indices]
    y_unclassified = y[~confident_indices]

    print(f"Total samples: {len(X)}")
    print(f"Classified samples (high confidence): {len(X_classified)}")
    print(f"Unclassified samples (low confidence): {len(X_unclassified)}")

    return X_classified, y_classified, X_unclassified, y_unclassified, y_pred_confident

# --- Main Pipeline Function ---

def cascaded_xgboost_pipeline(
    X_initial: pd.DataFrame,
    y_initial: pd.Series,
    num_cascades: int,
    xgboost_param_grid: Dict[str, Any],
    test_size: float = 0.2,
    val_size: float = 0.2,
    random_state: Optional[int] = None
) -> Dict[str, Any]:
    """
    Implements a cascaded XGBoost pipeline for binary predictions.

    Args:
        X_initial (pd.DataFrame): Initial features DataFrame.
        y_initial (pd.Series): Initial target Series.
        num_cascades (int): The number of XGBoost models in the cascade.
        xgboost_param_grid (Dict[str, Any]): Parameter grid for GridSearchCV for XGBoost.
        test_size (float): Proportion of the dataset for the final test set (default: 0.2).
        val_size (float): Proportion of the dataset for the validation set (default: 0.2).
        random_state (Optional[int]): Random state for reproducibility.

    Returns:
        Dict[str, Any]: A dictionary containing results from each cascade level, including:
            - 'models': List of trained XGBoost models.
            - 'confidence_thresholds': List of optimal confidence thresholds.
            - 'classified_data_per_cascade': List of (X_classified, y_classified, y_pred_confident) for each cascade.
            - 'unclassified_data_final': The remaining unclassified data after all cascades.
    """
    results = {
        'models': [],
        'confidence_thresholds': [],
        'classified_data_per_cascade': [],
        'unclassified_data_final': None
    }

    # Preprocess initial features to ensure numeric dtypes
    X_initial_processed = _preprocess_features(X_initial)
    current_X = X_initial_processed.copy()
    current_y = y_initial.copy()

    # Store original indices to track data through cascades
    current_indices = X_initial_processed.index.to_series()

    print(f"Starting cascaded XGBoost pipeline with {num_cascades} cascades.")

    for i in range(num_cascades):
        print(f"\n--- Cascade Level {i + 1} ---")

        if len(current_X) == 0:
            print("No data remaining to classify. Stopping cascade.")
            break

        # Step 1: Data Splitting (60-20-20 train-val-test)
        # Note: We re-split at each level from the *current* unclassified data.
        X_train, X_val, X_test, y_train, y_val, y_test = split_data(
            current_X, current_y, test_size=test_size, val_size=val_size, random_state=random_state + i if random_state is not None else None
        )

        # Step 2: Train and Tune XGBoost Model
        model = train_and_tune_xgboost(
            X_train, y_train,
            X_val, y_val,
            xgboost_param_grid,
            random_state=random_state
        )
        results['models'].append(model)

        # Step 3: Optimize Confidence Threshold on Validation Set
        confidence_threshold = optimize_confidence_threshold(model, X_val, y_val)
        results['confidence_thresholds'].append(confidence_threshold)

        # Step 4: Make Confident Predictions and Identify Unclassified Data
        # Apply the model and threshold to the *entire* current dataset (train+val+test combined)
        # to determine which samples are classified and which are passed to the next stage.
        X_classified_current, y_classified_current, X_unclassified_next, y_unclassified_next, y_pred_confident_current = \
            make_confident_predictions(model, current_X, current_y, confidence_threshold)

        # Store results for this cascade level
        results['classified_data_per_cascade'].append({
            'X_classified': X_classified_current,
            'y_classified': y_classified_current,
            'y_pred_confident': y_pred_confident_current,
            'cascade_level': i + 1
        })

        # Update current data for the next cascade
        current_X = X_unclassified_next
        current_y = y_unclassified_next

        print(f"End of Cascade Level {i + 1}. Remaining unclassified samples: {len(current_X)}")

    results['unclassified_data_final'] = {'X': current_X, 'y': current_y}
    print("\nCascaded pipeline finished.")
    return results

# --- New Helper Functions for Analysis and Evaluation ---

def evaluate_model(pipeline_results: Dict[str, Any], X_initial: pd.DataFrame, y_initial: pd.Series):
    """
    Evaluates the performance of the cascaded XGBoost pipeline at each level and overall.

    Args:
        pipeline_results (Dict[str, Any]): The results dictionary from cascaded_xgoost_pipeline.
        X_initial (pd.DataFrame): The initial features DataFrame.
        y_initial (pd.Series): The initial target Series.
    """
    print("\n--- Model Evaluation ---")

    all_classified_y_true = pd.Series(dtype=int)
    all_classified_y_pred = pd.Series(dtype=int)

    for i, cascade_data in enumerate(pipeline_results['classified_data_per_cascade']):
        print(f"\n--- Cascade Level {i + 1} Evaluation (on Classified Samples) ---")
        X_classified = cascade_data['X_classified']
        y_classified = cascade_data['y_classified']
        y_pred_confident = cascade_data['y_pred_confident'] # This is already a Series from make_confident_predictions

        if len(y_classified) > 0:
            print(f"Samples classified at this level: {len(y_classified)}")
            print("Classification Report:")
            print(classification_report(y_classified, y_pred_confident, zero_division=0))
            try:
                # Ensure y_pred_confident is treated as binary for ROC AUC
                roc_auc = roc_auc_score(y_classified, y_pred_confident)
                print(f"ROC AUC Score: {roc_auc:.4f}")
            except ValueError:
                print("ROC AUC cannot be calculated for this level (e.g., only one class present).")

            all_classified_y_true = pd.concat([all_classified_y_true, y_classified])
            all_classified_y_pred = pd.concat([all_classified_y_pred, y_pred_confident])
        else:
            print("No samples were classified at this level.")

    print("\n--- Overall Model Performance (on All Classified Samples) ---")
    if len(all_classified_y_true) > 0:
        overall_precision = precision_score(all_classified_y_true, all_classified_y_pred, zero_division=0)
        overall_coverage = len(all_classified_y_true) / len(X_initial)
        print(f"Total samples classified across all cascades: {len(all_classified_y_true)}")
        print(f"Overall Coverage (proportion of initial data classified): {overall_coverage:.4f}")
        print("Overall Classification Report:")
        print(classification_report(all_classified_y_true, all_classified_y_pred, zero_division=0))
        try:
            overall_roc_auc = roc_auc_score(all_classified_y_true, all_classified_y_pred)
            print(f"Overall ROC AUC Score: {overall_roc_auc:.4f}")
        except ValueError:
            print("Overall ROC AUC cannot be calculated (e.g., only one class present in classified data).")
    else:
        print("No samples were classified across the entire pipeline.")

    final_unclassified_X = pipeline_results['unclassified_data_final']['X']
    final_unclassified_y = pipeline_results['unclassified_data_final']['y']
    print(f"\n--- Unclassified Data Summary ---")
    print(f"Final unclassified samples after all cascades: {len(final_unclassified_X)}")
    if len(final_unclassified_y) > 0:
        print(f"Distribution of final unclassified samples:\n{final_unclassified_y.value_counts()}")
    else:
        print("All samples were classified by the pipeline.")


def plot_analysis(pipeline_results: Dict[str, Any], X_initial: pd.DataFrame, y_initial: pd.Series):
    """
    Generates various plots for analysis of the cascaded XGBoost pipeline.

    Args:
        pipeline_results (Dict[str, Any]): The results dictionary from cascaded_xgboost_pipeline.
        X_initial (pd.DataFrame): The initial features DataFrame.
        y_initial (pd.Series): The initial target Series.
    """
    print("\n--- Generating Plots ---")

    # 1. Mean Feature Importance Plot
    if pipeline_results['models']:
        feature_importances = []
        # Use the preprocessed X_initial for feature names consistency
        X_initial_processed_for_plotting = _preprocess_features(X_initial)

        for model in pipeline_results['models']:
            # Ensure feature names are consistent
            if hasattr(model, 'feature_importances_') and len(model.feature_importances_) == len(X_initial_processed_for_plotting.columns):
                feature_importances.append(model.feature_importances_)
            else:
                print(f"Warning: Could not get feature importances for a model or feature count mismatch. Skipping.")
                continue

        if feature_importances:
            mean_importances = np.mean(feature_importances, axis=0)
            importance_df = pd.DataFrame({
                'Feature': X_initial_processed_for_plotting.columns,
                'Importance': mean_importances
            }).sort_values(by='Importance', ascending=False)

            plt.figure(figsize=(10, 6))
            sns.barplot(x='Importance', y='Feature', data=importance_df)
            plt.title('Mean Feature Importance Across Cascades')
            plt.xlabel('Mean Importance')
            plt.ylabel('Feature')
            plt.tight_layout()
            plt.show()
        else:
            print("No valid feature importances found to plot.")
    else:
        print("No models trained to plot feature importance.")

    # 2. Confusion Matrix Plots (Per Cascade and Overall)
    for i, cascade_data in enumerate(pipeline_results['classified_data_per_cascade']):
        y_classified = cascade_data['y_classified']
        y_pred_confident = cascade_data['y_pred_confident'] # This is already a Series

        if len(y_classified) > 0:
            cm = confusion_matrix(y_classified, y_pred_confident)
            plt.figure(figsize=(6, 5))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
                        xticklabels=['Predicted 0', 'Predicted 1'],
                        yticklabels=['Actual 0', 'Actual 1'])
            plt.title(f'Confusion Matrix - Cascade Level {i + 1}')
            plt.xlabel('Predicted Label')
            plt.ylabel('True Label')
            plt.tight_layout()
            plt.show()
        else:
            print(f"No classified data for Confusion Matrix at Cascade Level {i + 1}.")

    # Overall Confusion Matrix
    all_classified_y_true = pd.Series(dtype=int)
    all_classified_y_pred = pd.Series(dtype=int)
    for cascade_data in pipeline_results['classified_data_per_cascade']:
        all_classified_y_true = pd.concat([all_classified_y_true, cascade_data['y_classified']])
        all_classified_y_pred = pd.concat([all_classified_y_pred, cascade_data['y_pred_confident']]) # This is already a Series

    if len(all_classified_y_true) > 0:
        cm_overall = confusion_matrix(all_classified_y_true, all_classified_y_pred)
        plt.figure(figsize=(6, 5))
        sns.heatmap(cm_overall, annot=True, fmt='d', cmap='Blues', cbar=False,
                    xticklabels=['Predicted 0', 'Predicted 1'],
                    yticklabels=['Actual 0', 'Actual 1'])
        plt.title('Overall Confusion Matrix (All Classified Samples)')
        plt.xlabel('Predicted Label')
        plt.ylabel('True Label')
        plt.tight_layout()
        plt.show()
    else:
        print("No overall classified data for Confusion Matrix.")

    # 3. Correlation Heatmap of Initial Features
    # Use the preprocessed X_initial for correlation calculation
    X_initial_processed_for_plotting = _preprocess_features(X_initial)
    if not X_initial_processed_for_plotting.empty:
        plt.figure(figsize=(12, 10))
        sns.heatmap(X_initial_processed_for_plotting.corr(), annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
        plt.title('Feature Correlation Heatmap (Initial Data)')
        plt.tight_layout()
        plt.show()
    else:
        print("No initial features to plot correlation heatmap after preprocessing.")

# --- New Function for Prediction on New Data ---

def predict_new_data(
    new_X: pd.DataFrame,
    pipeline_results: Dict[str, Any]
) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame]:
    """
    Makes predictions on new, unseen data using the trained cascaded XGBoost pipeline.

    Args:
        new_X (pd.DataFrame): Features of the new data to predict on.
        pipeline_results (Dict[str, Any]): The results dictionary from cascaded_xgboost_pipeline,
                                            containing trained models and confidence thresholds.

    Returns:
        Tuple[pd.DataFrame, pd.Series, pd.DataFrame]:
            classified_predictions_df (pd.DataFrame): DataFrame of classified samples with their predictions.
                                                      Includes original index, features, and predicted label.
            unclassified_X (pd.DataFrame): Features of samples that remained unclassified after all cascades.
            all_predictions_series (pd.Series): Series of all predictions (including unconfident ones),
                                                indexed by original new_X index. Unconfident predictions will be -1.
    """
    print("\n--- Making predictions on new data ---")

    # Preprocess new_X to ensure numeric dtypes, consistent with training data
    new_X_processed = _preprocess_features(new_X)
    current_X_to_predict = new_X_processed.copy()

    # Store all predictions, initialized with NaN
    all_predictions_series = pd.Series(np.nan, index=new_X_processed.index, dtype=float)

    classified_data_list = [] # To store classified samples from each cascade

    for i, model in enumerate(pipeline_results['models']):
        print(f"Applying Cascade Level {i + 1} model to remaining data ({len(current_X_to_predict)} samples)...")
        if len(current_X_to_predict) == 0:
            print("No data remaining to predict. Stopping.")
            break

        confidence_threshold = pipeline_results['confidence_thresholds'][i]

        # Get probabilities from the current model
        y_pred_proba_current = model.predict_proba(current_X_to_predict)[:, 1]

        # Identify confident predictions (above threshold or below 1 - threshold)
        confident_indices = (y_pred_proba_current >= confidence_threshold) | (y_pred_proba_current <= (1 - confidence_threshold))

        # Samples classified by this model
        X_classified_by_current = current_X_to_predict[confident_indices]
        y_pred_classified_by_current = (y_pred_proba_current[confident_indices] >= 0.5).astype(int)

        # Update the overall predictions series
        all_predictions_series.loc[X_classified_by_current.index] = y_pred_classified_by_current

        # Store classified data for the final DataFrame
        if not X_classified_by_current.empty:
            classified_df = X_classified_by_current.copy()
            classified_df['predicted_label'] = y_pred_classified_by_current
            classified_df['cascade_level_classified'] = i + 1
            classified_data_list.append(classified_df)

        # Update current_X_to_predict to only include unclassified samples for the next cascade
        current_X_to_predict = current_X_to_predict[~confident_indices]

    # Combine all classified data into a single DataFrame
    classified_predictions_df = pd.DataFrame()
    if classified_data_list:
        classified_predictions_df = pd.concat(classified_data_list)
        # Ensure unique index in case of overlapping indices (though unlikely with time series)
        classified_predictions_df = classified_predictions_df[~classified_predictions_df.index.duplicated(keep='first')]

    # Assign -1 to unclassified samples in the all_predictions_series
    all_predictions_series = all_predictions_series.fillna(-1).astype(int)

    print(f"Total samples processed: {len(new_X)}")
    print(f"Total samples classified with high confidence: {len(classified_predictions_df)}")
    print(f"Total samples remaining unclassified: {len(current_X_to_predict)}")

    return classified_predictions_df, current_X_to_predict, all_predictions_series

X_data = train_data_ti.drop(['Open','Close','High', 'Low', 'Volume', 'Return', 'Return', 'Intraday_trade'], axis = 1)
y_data = train_data_ti['Intraday_trade']

# 2. Define XGBoost Hyperparameter Grid
# This is a small grid for demonstration purposes. In a real scenario,
# you'd have a much larger and more diverse grid.
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [3, 5],
    'learning_rate': [0.1, 0.05],
    'subsample': [0.8],
    'colsample_bytree': [0.8]
}

# 3. Define the number of cascades
num_cascades_to_run = 3 # You can change this to test more or fewer cascades

# 4. Run the Cascaded XGBoost Pipeline
pipeline_results = cascaded_xgboost_pipeline(
    X_initial=X_data,
    y_initial=y_data,
    num_cascades=num_cascades_to_run,
    xgboost_param_grid=param_grid,
    test_size=0.2,
    val_size=0.2,
    random_state=42
)

# 5. Evaluate the Model
evaluate_model(pipeline_results, X_data, y_data)

# 6. Plot Analysis
plot_analysis(pipeline_results, X_data, y_data)

test_data_ti = functions_algtg.technical_indicators(test_data, short_window = 5, long_window = 26)

test_data_ti['Return'] = (test_data_ti['Close'] - test_data_ti['Open']) / test_data_ti['Open']

test_data_ti['Intraday_trade'] = 0
test_data_ti.loc[test_data_ti['Return'] > 0.02, 'Intraday_trade'] = 1

X_data_test = test_data_ti.drop(['Open','Close','High', 'Low', 'Volume', 'Return', 'Return', 'Intraday_trade'], axis = 1)
y_data_test = test_data_ti['Intraday_trade']

classified_preds_df, unclassified_new_X, all_preds_series = predict_new_data(X_data_test, pipeline_results)

classified_preds_df.head()

test_data_ti['pred_intraday_signal'] = classified_preds_df['predicted_label']

test_data_ti['pred_intraday_signal'].value_counts()

test_data_ti['strategy_daily_return'] = test_data_ti['pred_intraday_signal'] * test_data_ti['Return']

test_data_ti['synthetic_price'] = (1 + test_data_ti['strategy_daily_return']).cumprod()

test_data_ti['actual_daily_ret_for_signal_hit_miss'] = test_data_ti['Return']

df_for_backtest = test_data_ti[['Close', 'Open','High','Low', 'pred_intraday_signal', 'synthetic_price', 'actual_daily_ret_for_signal_hit_miss']].copy()

df_for_backtest['pred_intraday_signal'].value_counts()

# df_original = test_data_ti.copy()
# df_original['oc_return'] = df_original['Close'] / df_original['Open'] - 1

# # Create a 'synthetic_price' column.
# # This synthetic price should represent the cumulative growth of the asset's
# # open-to-close movements, regardless of your signal.
# # The signal column will then multiply this *within* the backtester.
# df_original['synthetic_price'] = (1 + df_original['oc_return']).cumprod()
# # Adjust the initial value to match the original price level if desired, e.g., df_original['synthetic_price'] *= df_original['open'].iloc[0] / df_original['synthetic_price'].iloc[0]
# # Or simply start from 1.0 (as cumprod does by default on returns)

# # For hit/miss statistics, the `returns` argument should be the actual market movement
# # the trade was exposed to (i.e., open-to-close returns).
# df_original['actual_daily_ret_for_signal_hit_miss'] = df_original['oc_return']

# # Create a copy of df_original that only contains columns the backtester expects
# # We need:
# # - 'close': For the Buy & Hold benchmark calculation (df['close'].pct_change())
# # - 'synthetic_price': As the `price_col` for the strategy's equity curve
# # - 'signal': The original signal column
# # - 'actual_daily_ret_for_signal_hit_miss': For _calculate_hit_miss_stats's 'returns' parameter
# # - 'open' is not strictly needed by run_backtest, but good to keep for reference if needed elsewhere.

# df_for_backtest = df_original[['Close', 'Open', 'pred_intraday_signal', 'synthetic_price', 'actual_daily_ret_for_signal_hit_miss']].copy()

# print("\nDataFrame Prepared for Backtest:")
# print(df_for_backtest.head())
# print("-" * 30)

# # 3. Run the Backtest with the Prepared DataFrame

# # Key parameters for your setup:
# # - price_col: Now 'synthetic_price'
# # - execution_lag: Set to 0. Your signal for day T means you trade on day T.
# # - cost_bps: Your transaction costs.
# # - signal_col: Your original 'signal' column.

# output_directory = Path("./backtest_results_open_close_strategy") # New directory
# output_directory.mkdir(exist_ok=True)

# try:
#     result = run_backtest(
#         df=df_for_backtest,
#         price_col="synthetic_price",  # This column's pct_change() will be the asset_ret in run_backtest
#         signal_col="pred_intraday_signal",
#         rf=0.0,
#         periods_per_year=252, # Assuming daily data
#         cost_bps=10,          # 10 bps per unit of turnover (e.g., 0.1% of value traded)
#         execution_lag=0,      # Signal for today means trade today.
#         rolling_window=20,    # ~1 month for daily data
#         signal_threshold=0.01,# Minimum position change to count as a signal (e.g., from 0 to 1)
#         out_dir=output_directory,
#         prefix="daily_oc_strategy",
#         make_plots=True,
#         report_title="Daily Open-to-Close Strategy (Corrected)",
#         report_description="Performance of a strategy that buys at open and sells at close based on a daily signal.",
#         report_author="Your Name/Team",
#         report_notes="This backtest uses a synthetic price series whose returns are the open-to-close returns, allowing the signal to correctly modulate them within the unchanged backtester logic."
#     )

#     print("\nBacktest Results (Corrected):")
#     print(result.metrics)
#     print("\nHit/Miss Statistics (Corrected):")
#     print(result.hit_miss_stats)
#     print("\nFirst 5 Strategy Returns (Net, Corrected):")
#     print(result.returns['strategy_ret'].head())
#     print("\nFirst 5 Buy & Hold Returns:")
#     print(result.returns['bh_ret'].head())
#     print("\nFirst 5 Position values:")
#     print(result.returns['position'].head())
#     print("\nFirst 5 Turnover values:")
#     print(result.returns['turnover'].head())

#     # Generate the HTML report
#     html_report_path = generate_html_report(result)
#     print(f"HTML report saved to: {html_report_path}")

# except Exception as e:
#     print(f"An error occurred during backtest: {e}")

from intraday_backtester import IntradayBacktester, generate_html_report
from pathlib import Path

output_directory_daily_oc_custom = Path("./intraday_backtest_results_daily_oc_custom_cols")
output_directory_daily_oc_custom.mkdir(exist_ok=True)

print("\nRunning Backtest for 'Open-to-Close' Daily Strategy (Custom Columns):")
try:
    backtester_daily_oc_custom = IntradayBacktester(
        df=df_for_backtest,
        open_col="Open",    # Specify your custom column names here
        high_col="High",
        low_col="Low",
        close_col="Close",
        signal_col="pred_intraday_signal",   # Specify your custom signal column name here
        rf=0.0,
        periods_per_year=252,
        cost_bps=5,
        execution_lag=0,
        rolling_window=20,
        entry_exit_model='open_to_close_bar',
        report_title="Daily Open-to-Close Strategy (Custom Columns)",
        report_description="Performance of a daily strategy that buys at open and sells at close using custom column names.",
        report_author="Intraday Team",
        report_notes="This backtest uses the 'open_to_close_bar' model with daily OHLC data and custom column names."
    )
    result_daily_oc_custom = backtester_daily_oc_custom.run(out_dir=output_directory_daily_oc_custom, prefix="daily_oc_strategy_custom", make_plots=True)

    print("\n--- Daily Open-to-Close Strategy Results (Custom Columns) ---")
    print(result_daily_oc_custom.metrics)
    print("\nHit/Miss Statistics:")
    print(result_daily_oc_custom.hit_miss_stats)
    print("\nFirst 5 Strategy Returns (Net):")
    print(result_daily_oc_custom.returns['strategy_ret_net'].head())

    html_report_path_daily_oc_custom = generate_html_report(result_daily_oc_custom)
    print(f"HTML report saved to: {html_report_path_daily_oc_custom}")

except Exception as e:
    print(f"An error occurred during Daily Open-to-Close backtest (Custom Columns): {e}")

